# -*- coding: utf-8 -*-
"""HOG+LBP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JNtk0CTe_dZRGX2lCh8EPC-AlYSlZ0XZ
"""

# =========================
# Core
# =========================
import os
import json
import shutil
import random
import pickle
from collections import defaultdict

# =========================
# Numeric & Visualization
# =========================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm

# =========================
# Computer Vision
# =========================
import cv2

# =========================
# Feature Extraction
# =========================
from skimage.feature import hog, local_binary_pattern

# =========================
# Machine Learning
# =========================
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score

# =========================
# Utils
# =========================
import joblib

BASE_DIR = "/content/data"

ANN_DIR = os.path.join(BASE_DIR, "annotations")
IMG_DIR = os.path.join(BASE_DIR, "train2017")
TEST_DIR = os.path.join(BASE_DIR, "test2017")

ANN_PATH = os.path.join(ANN_DIR, "instances_train2017.json")

OUTPUT_DIR = os.path.join(BASE_DIR, "output")
SUBSET_DIR = os.path.join(OUTPUT_DIR, "subset")
SUBSET_IMG = os.path.join(SUBSET_DIR, "images")
SUBSET_ANN = os.path.join(SUBSET_DIR, "instances_train2017_subset.json")

for d in [SUBSET_IMG]:
    os.makedirs(d, exist_ok=True)

print("✅ Paths ready")

import requests

ANNOT_URL = "http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
ANNOT_ZIP = "/content/data/annotations_trainval2017.zip"

print("Downloading annotations...")
r = requests.get(ANNOT_URL, stream=True)
with open(ANNOT_ZIP, "wb") as f:
    for chunk in r.iter_content(chunk_size=1024):
        if chunk:
            f.write(chunk)

print("Annotation ZIP downloaded.")

import zipfile

with zipfile.ZipFile(ANNOT_ZIP, "r") as zip_ref:
    zip_ref.extractall("/content/data")

print("Annotation extracted.")

TRAIN_URL = "http://images.cocodataset.org/zips/train2017.zip"
TRAIN_ZIP = "/content/data/train2017.zip"

print("Downloading train images (this will take time)...")
r = requests.get(TRAIN_URL, stream=True)
with open(TRAIN_ZIP, "wb") as f:
    for chunk in r.iter_content(chunk_size=1024):
        if chunk:
            f.write(chunk)

print("Train images ZIP downloaded.")

import requests
import os

TEST_URL = "http://images.cocodataset.org/zips/test2017.zip"
TEST_ZIP = "/content/data/test2017.zip"

print("Downloading test2017 images...")
r = requests.get(TEST_URL, stream=True)
with open(TEST_ZIP, "wb") as f:
    for chunk in r.iter_content(chunk_size=1024):
        if chunk:
            f.write(chunk)

print("test2017 ZIP downloaded.")

with zipfile.ZipFile(TRAIN_ZIP, "r") as zip_ref:
    zip_ref.extractall("/content/data")

print("Train images extracted.")

with zipfile.ZipFile(TEST_ZIP, "r") as zip_ref:
    zip_ref.extractall("/content/data")

print("Test Image extracted.")

import json, random
from collections import defaultdict

# =========================
# CONFIG
# =========================

SUBSET_SIZE = 10000
TARGET_CATS = {1, 2, 3, 18}   # person, bicycle, car, dog
MAX_PER_CAT = SUBSET_SIZE // len(TARGET_CATS)  # 2500 per class

# =========================
# LOAD COCO ANNOTATION
# =========================

with open(ANN_PATH, "r") as f:
    coco = json.load(f)

# =========================
# FILTER ANNOTATIONS BY TARGET CATEGORIES
# =========================

filtered_ann = [
    ann for ann in coco["annotations"]
    if ann["category_id"] in TARGET_CATS
]

print("Filtered annotations:", len(filtered_ann))

# =========================
# BALANCED SAMPLING PER CATEGORY
# =========================

by_cat = defaultdict(list)
for ann in filtered_ann:
    by_cat[ann["category_id"]].append(ann)

subset_ann = []
for cat_id in TARGET_CATS:
    anns = by_cat[cat_id]
    random.shuffle(anns)
    subset_ann.extend(anns[:MAX_PER_CAT])
    print(f"Category {cat_id}: {len(anns[:MAX_PER_CAT])} annotations")

# =========================
# COLLECT CORRESPONDING IMAGES
# =========================

img_ids = set(a["image_id"] for a in subset_ann)

subset_imgs = [
    img for img in coco["images"]
    if img["id"] in img_ids
]

print("Unique images in subset:", len(subset_imgs))

# =========================
# SAVE SUBSET ANNOTATION
# =========================

subset_coco = {
    "images": subset_imgs,
    "annotations": subset_ann,
    "categories": coco["categories"]
}

with open(SUBSET_ANN, "w") as f:
    json.dump(subset_coco, f)

print(f"✅ Subset annotation saved: {SUBSET_ANN}")

id_to_name = {img["id"]: img["file_name"] for img in subset_imgs}

for fname in tqdm(id_to_name.values(), desc="Copy subset images"):
    src = os.path.join(IMG_DIR, fname)
    dst = os.path.join(SUBSET_IMG, fname)
    if os.path.exists(src):
        shutil.copy(src, dst)

from collections import Counter

image_counts = Counter(a["image_id"] for a in subset_ann)

print("Max object in one image :", max(image_counts.values()))
print("Avg object per image    :", sum(image_counts.values()) / len(image_counts))

def coco_to_object_pairs(img_dir, ann_path):
    with open(ann_path) as f:
        data = json.load(f)

    img_map = {i["id"]: i for i in data["images"]}
    grouped = {}

    for ann in data["annotations"]:
        img_id = ann["image_id"]
        if img_id not in img_map:
            continue

        img_path = os.path.join(img_dir, img_map[img_id]["file_name"])
        if not os.path.exists(img_path):
            continue

        grouped.setdefault(img_id, {"img": img_path, "boxes": [], "labels": []})
        grouped[img_id]["boxes"].append(ann["bbox"])
        grouped[img_id]["labels"].append(ann["category_id"])

    pairs = []
    for v in grouped.values():
        areas = [w*h for (_,_,w,h) in v["boxes"]]
        idx = np.argmax(areas)
        pairs.append({
            "img": v["img"],
            "bbox": v["boxes"][idx],
            "label": v["labels"][idx]
        })

    return pairs


SELECTED = {1, 2, 3, 18}  # person, bicycle, car, dog
pairs = coco_to_object_pairs(SUBSET_IMG, SUBSET_ANN)
pairs = [p for p in pairs if p["label"] in SELECTED]

print("Total object-centric samples:", len(pairs))

from sklearn.model_selection import train_test_split

image_ids = list(set(p["img"] for p in pairs))

train_imgs, val_imgs = train_test_split(
    image_ids, test_size=0.2, random_state=42
)

train_pairs = [p for p in pairs if p["img"] in train_imgs]
val_pairs   = [p for p in pairs if p["img"] in val_imgs]

print("Train pairs:", len(train_pairs))
print("Val pairs  :", len(val_pairs))

TARGET = 3000

def oversample(pairs):
    grouped = defaultdict(list)
    for p in pairs:
        grouped[p["label"]].append(p)

    balanced = []
    for lbl, items in grouped.items():
        if len(items) >= TARGET:
            balanced.extend(items[:TARGET])
        else:
            k = TARGET // len(items)
            r = TARGET % len(items)
            balanced.extend(items * k)
            balanced.extend(random.sample(items, r))
    return balanced

train_balanced = oversample(train_pairs)
print("Balanced train samples:", len(train_balanced))

def augment_safe(img, label):
    if label == 1:  # person
        return img

    if random.random() < 0.5:
        img = cv2.flip(img, 1)

    if random.random() < 0.4:
        h, w = img.shape[:2]
        angle = random.uniform(-10, 10)
        M = cv2.getRotationMatrix2D((w//2, h//2), angle, 1)
        img = cv2.warpAffine(img, M, (w,h), borderMode=cv2.BORDER_REFLECT_101)

    return img

from skimage.feature import hog, local_binary_pattern
import numpy as np
import cv2

LBP_P = 8
LBP_R = 1
LBP_BINS = LBP_P + 2  # WAJIB TETAP (10)

def extract_feature(img_path, bbox, label):
    img = cv2.imread(img_path)
    x, y, w, h = map(int, bbox)

    crop = img[y:y+h, x:x+w]
    crop = cv2.resize(crop, (256, 256))

    crop = augment_safe(crop, label)

    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)

    hog_feat = hog(
        gray,
        orientations=9,
        pixels_per_cell=(8, 8),
        cells_per_block=(2, 2),
        block_norm="L2-Hys"
    )

    lbp = local_binary_pattern(gray, P=LBP_P, R=LBP_R, method="uniform")

    lbp_hist, _ = np.histogram(
        lbp.ravel(),
        bins=LBP_BINS,
        range=(0, LBP_BINS),
        density=True
    )

    return np.concatenate([hog_feat, lbp_hist])

def extract_feature_test(img_path, bbox):
    img = cv2.imread(img_path)
    x,y,w,h = map(int, bbox)
    crop = cv2.resize(img[y:y+h, x:x+w], (256,256))

    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)

    hog_feat = hog(gray, orientations=9,
                   pixels_per_cell=(8,8),
                   cells_per_block=(2,2),
                   block_norm="L2-Hys")

    lbp = local_binary_pattern(gray, 8, 1, method="uniform")
    hist,_ = np.histogram(lbp.ravel(), bins=LBP_BINS, range=(0,LBP_BINS), density=True)

    return np.concatenate([hog_feat, hist])

from collections import Counter

dist = Counter(p["label"] for p in train_pairs)
print(dist)
dist2 = Counter(p["label"] for p in val_pairs)
print(dist2)

X_train, y_train = [], []
for p in tqdm(train_balanced):
    X_train.append(extract_feature(p["img"], p["bbox"], p["label"]))
    y_train.append(p["label"])

X_val, y_val = [], []
for p in tqdm(val_pairs):
    X_val.append(extract_feature_test(p["img"], p["bbox"]))
    y_val.append(p["label"])

X_train = np.array(X_train)
X_val   = np.array(X_val)
y_train = np.array(y_train)
y_val   = np.array(y_val)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val   = scaler.transform(X_val)

PCA(n_components=0.96, svd_solver="randomized", random_state=42)
X_train_pca = pca.fit_transform(X_train)
X_val_pca   = pca.transform(X_val)

print("PCA dim:", X_train_pca.shape[1])

from sklearn.metrics import f1_score

best_C = None
best_f1 = 0

for C in [0.5, 1.0, 2.0]:
    clf_tmp = LinearSVC(C=C, class_weight="balanced", max_iter=5000)
    clf_tmp.fit(X_train_pca, y_train)

    y_pred = clf_tmp.predict(X_val_pca)
    f1 = f1_score(y_val, y_pred, average="macro")

    print(f"C={C} → Macro F1={f1:.3f}")

    if f1 > best_f1:
        best_f1 = f1
        best_C = C

print("Best C:", best_C)

clf = LinearSVC(C=best_C, class_weight="balanced", max_iter=5000)
clf.fit(X_train_pca, y_train)

y_val_pred = clf.predict(X_val_pca)

label_map = {1:"person", 2:"bicycle", 3:"car", 18:"dog"}

print(classification_report(
    y_val, y_val_pred,
    target_names=[label_map[c] for c in sorted(label_map)]
))

import joblib
import os

SAVE_DIR = os.path.join(OUTPUT_DIR, "models")
os.makedirs(SAVE_DIR, exist_ok=True)

joblib.dump(clf,    os.path.join(SAVE_DIR, "svm_hoglbp_new.pkl"))
joblib.dump(scaler, os.path.join(SAVE_DIR, "scaler_new.pkl"))
joblib.dump(pca,    os.path.join(SAVE_DIR, "pca_new.pkl"))

print("✅ Model artifacts saved")

"""Interferencd With YOLO"""

from ultralytics import YOLO
yolo = YOLO("yolov8n.pt")

def predict_test_image(img_path):
    res = yolo(img_path, conf=0.4, verbose=False)
    if len(res[0].boxes) == 0:
        return "No object"

    boxes = res[0].boxes.xyxy.cpu().numpy()
    areas = (boxes[:,2]-boxes[:,0])*(boxes[:,3]-boxes[:,1])
    i = areas.argmax()

    x1,y1,x2,y2 = boxes[i]
    bbox = (int(x1), int(y1), int(x2-x1), int(y2-y1))

    feat = extract_feature_test(img_path, bbox)
    feat = scaler.transform(feat.reshape(1,-1))
    feat = pca.transform(feat)

    pred = clf.predict(feat)[0]
    return label_map[pred]

img_name = random.choice(os.listdir(TEST_DIR))
print(img_name, "→", predict_test_image(os.path.join(TEST_DIR, img_name)))